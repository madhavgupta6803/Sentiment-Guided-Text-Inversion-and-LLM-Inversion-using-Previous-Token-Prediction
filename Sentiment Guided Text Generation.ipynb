{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11580768,"sourceType":"datasetVersion","datasetId":7261239},{"sourceId":359218,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":299140,"modelId":319733}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport re\nimport os\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm\n\n# Download NLTK resources\nnltk.download('punkt_tab')\nnltk.download('punkt', quiet=True)\n\n# Text preprocessing\ndef preprocess_text(text):\n    \"\"\"Basic preprocessing for social media text\"\"\"\n    if isinstance(text, str):\n        text = text.lower()\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'@\\S+', '@user', text)\n        text = re.sub(r'#(\\S+)', r'\\1', text)\n        return text\n    return \"\"\n\n# Dataset class for text generation\nclass TextGenerationDataset(Dataset):\n    def __init__(self, texts, labels, vocab=None, max_len=30):\n        self.texts = texts\n        self.labels = labels\n        self.max_len = max_len\n\n        if vocab is None:\n            self.build_vocab()\n        else:\n            self.vocab = vocab\n            self.idx_to_word = {v: k for k, v in vocab.items()}\n            self.vocab_size = len(vocab)\n\n    def build_vocab(self):\n        word_counts = Counter()\n        for text in self.texts:\n            tokens = word_tokenize(preprocess_text(text))\n            word_counts.update(tokens)\n\n        self.vocab = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n        for word, _ in word_counts.items():\n            if word not in self.vocab:\n                self.vocab[word] = len(self.vocab)\n\n        self.idx_to_word = {v: k for k, v in self.vocab.items()}\n        self.vocab_size = len(self.vocab)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        tokens = ['<START>'] + word_tokenize(preprocess_text(text)) + ['<END>']\n        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens[:self.max_len]]\n\n        if len(indices) < self.max_len:\n            indices += [self.vocab['<PAD>']] * (self.max_len - len(indices))\n        else:\n            indices = indices[:self.max_len]\n\n        one_hot = torch.zeros(4)\n        one_hot[label] = 1\n\n        return torch.tensor(indices, dtype=torch.long), one_hot\n\n\nclass TextGenerator(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim, output_dim, max_length, num_layers=4, num_heads=8, dropout=0.1):\n        super(TextGenerator, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        self.output_dim = output_dim\n        self.max_length = max_length\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n\n        # Word embeddings\n        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n\n        # Positional encodings\n        self.pos_embedding = nn.Embedding(max_length, embedding_dim)\n        self.scale = torch.sqrt(torch.tensor(embedding_dim, dtype=torch.float32))\n\n        # Sentiment conditioning\n        self.class_embedding = nn.Linear(output_dim, latent_dim)\n        self.latent_to_hidden = nn.Linear(latent_dim * 2, hidden_dim)\n        self.class_to_embedding = nn.Linear(latent_dim, embedding_dim)\n\n        # Custom transformer decoder layers\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embedding_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.decoder_layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n\n        # Output projection\n        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_square_subsequent_mask(self, sz):\n        \"\"\"Generate a causal mask to prevent attending to future tokens.\"\"\"\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n        return mask\n\n    def forward(self, latent_vector, conditioning_vector, input_tokens=None):\n        batch_size = latent_vector.size(0)\n        device = latent_vector.device\n\n        # Sentiment embedding\n        class_embedding = self.class_embedding(conditioning_vector)  # (batch_size, latent_dim)\n        combined = torch.cat([latent_vector, class_embedding], dim=1)  # (batch_size, latent_dim * 2)\n        hidden = self.latent_to_hidden(combined)  # (batch_size, hidden_dim)\n        class_embed = self.class_to_embedding(class_embedding)  # (batch_size, embedding_dim)\n\n        if input_tokens is None:\n            # During inference, start with <START> token\n            input_tokens = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)  # <START> token\n\n        seq_len = input_tokens.size(1)\n        positions = torch.arange(0, seq_len, device=device).unsqueeze(0).repeat(batch_size, 1)\n        token_embeds = self.word_embedding(input_tokens) * self.scale\n        pos_embeds = self.pos_embedding(positions)\n\n        # Add sentiment embedding to token embeddings\n        sentiment_embeds = class_embed.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, embedding_dim)\n        decoder_input = self.dropout(token_embeds + pos_embeds + sentiment_embeds)\n\n        # Create causal mask\n        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(device)\n\n        # Dummy memory tensor (same shape as input, but not used for cross-attention)\n        memory = torch.zeros_like(decoder_input)  # (batch_size, seq_len, embedding_dim)\n\n        # Pass through custom decoder layers\n        output = decoder_input\n        for layer in self.decoder_layers:\n            output = layer(output, memory=memory, tgt_mask=tgt_mask)  # Use dummy memory\n\n        # Project to vocabulary\n        logits = self.output_projection(output)  # (batch_size, seq_len, vocab_size)\n        return logits\n\n    def sample(self, latent_vector, conditioning_vector, temperature=1.0, max_length=None):\n        if max_length is None:\n            max_length = self.max_length\n        batch_size = latent_vector.size(0)\n        device = latent_vector.device\n        self.eval()\n\n        # Start with <START> token\n        generated = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)  # (batch_size, 1)\n\n        with torch.no_grad():\n            for t in range(max_length - 1):\n                logits = self(latent_vector, conditioning_vector, generated)  # (batch_size, seq_len, vocab_size)\n                logits = logits[:, -1, :]  # Take logits of the last token (batch_size, vocab_size)\n\n                if temperature != 1.0:\n                    logits = logits / temperature\n\n                probs = F.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n\n                generated = torch.cat([generated, next_token], dim=1)  # (batch_size, seq_len + 1)\n\n                # Stop if <END> token is generated\n                if (next_token == 2).all():  # <END> token\n                    break\n\n        return generated\n\n# Sentiment Classifier (Updated with hooks)\nclass SentimentClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n        super(SentimentClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embedding_dim, hidden_dim, num_layers=n_layers,\n            bidirectional=True, dropout=dropout if n_layers > 1 else 0, batch_first=True\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        return self.fc(hidden)\n\n    def register_hooks(self, hook_layers):\n        hooks, features = [], []\n        def hook_function_fc(module, input, output, features_list):\n            features_list.append(output.detach().clone())\n        def hook_function_lstm(module, input, output, features_list):\n            features_list.append(output[0].detach().clone())\n\n        for layer_idx in hook_layers:\n            if isinstance(layer_idx, int):\n                layer = self.get_layer_by_idx(layer_idx)\n                if layer is not None:\n                    features.append([])\n                    if layer_idx == 0:\n                        hooks.append(layer.register_forward_hook(\n                            lambda module, input, output, features_list=features[-1]: hook_function_fc(module, input, output, features_list)\n                        ))\n                    elif layer_idx == 1:\n                        hooks.append(layer.register_forward_hook(\n                            lambda module, input, output, features_list=features[-1]: hook_function_lstm(module, input, output, features_list)\n                        ))\n        return hooks, features\n\n    def get_layer_by_idx(self, idx):\n        if idx == 0:\n            return self.fc\n        elif idx == 1:\n            return self.lstm\n        return None\n\n# New Loss Functions\ndef cosine_similarity_loss(features):\n    if not features:\n        return torch.tensor(0.0, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n    if len(features[0].shape) > 2:\n        features_2d = features[0].mean(dim=1)\n    else:\n        features_2d = features[0]\n    normalized_features = F.normalize(features_2d, p=2, dim=1)\n    similarity_matrix = torch.mm(normalized_features, normalized_features.t())\n    mask = torch.eye(similarity_matrix.size(0), device=similarity_matrix.device).bool()\n    similarity_matrix = similarity_matrix.masked_fill(mask, 0)\n    loss = similarity_matrix.sum() / (similarity_matrix.size(0) * (similarity_matrix.size(0) - 1))\n    return loss\n\ndef feature_orthogonality_loss(features):\n    if not features:\n        return torch.tensor(0.0, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n    if len(features[0].shape) > 2:\n        features_2d = features[0].mean(dim=1)\n    else:\n        features_2d = features[0]\n    gram_matrix = torch.mm(features_2d, features_2d.t())\n    identity_matrix = torch.eye(gram_matrix.size(0), device=gram_matrix.device)\n    loss = torch.mean((gram_matrix - identity_matrix) ** 2)\n    return loss / (features_2d.size(0) * features_2d.size(1))\n\n# Updated Training Function\ndef train_generator(model, iterator, optimizer, criterion, classifier, device, lambda_sent=1.0, lambda_cos=0.5, lambda_orth=0.01, features=None):\n    epoch_loss, epoch_sent_loss, epoch_cos_loss, epoch_orth_loss = 0, 0, 0, 0\n    model.train()\n    classifier.train()\n\n    for batch in iterator:\n        texts, labels = batch\n        texts, labels = texts.to(device), labels.to(device)\n        batch_size = texts.size(0)\n        latent_vector = torch.randn(batch_size, model.latent_dim).to(device)\n\n        # Prepare input and target for teacher forcing\n        input_tokens = texts[:, :-1]  # Exclude last token\n        target_tokens = texts[:, 1:]  # Exclude first token (<START>)\n\n        logits = model(latent_vector, labels, input_tokens)  # (batch_size, seq_len-1, vocab_size)\n        logits_flat = logits.reshape(-1, model.vocab_size)\n        targets_flat = target_tokens.reshape(-1)\n        token_loss = criterion(logits_flat, targets_flat)\n\n        # Generate tokens for sentiment evaluation\n        generated_tokens = model.sample(latent_vector, labels)\n        sentiment_preds = classifier(generated_tokens)\n        sent_loss = F.cross_entropy(sentiment_preds, torch.argmax(labels, dim=1))\n\n        cos_loss = sum(cosine_similarity_loss(feat) for feat in features if feat)\n        orth_loss = sum(feature_orthogonality_loss(feat) for feat in features if feat)\n\n        total_loss = token_loss + lambda_sent * sent_loss + lambda_cos * cos_loss\n        # total_loss = token_loss + lambda_sent * sent_loss + lambda_cos * cos_loss + lambda_orth * orth_loss\n        optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        epoch_loss += token_loss.item()\n        epoch_sent_loss += sent_loss.item()\n        epoch_cos_loss += cos_loss.item()\n        epoch_orth_loss += orth_loss.item()\n\n        for feat in features:\n            feat.clear()\n\n    return (epoch_loss / len(iterator), epoch_sent_loss / len(iterator),\n            epoch_cos_loss / len(iterator), epoch_orth_loss / len(iterator))\n\n# Updated Evaluation Function\ndef evaluate_generator(model, iterator, criterion, classifier, device, features=None):\n    epoch_loss, total_correct, total_samples = 0, 0, 0\n    model.eval()\n    classifier.eval()\n\n    with torch.no_grad():\n        for batch in iterator:\n            texts, labels = batch\n            texts, labels = texts.to(device), labels.to(device)\n            batch_size = texts.size(0)\n            latent_vector = torch.randn(batch_size, model.latent_dim).to(device)\n\n            # Teacher forcing for token prediction loss\n            input_tokens = texts[:, :-1]\n            target_tokens = texts[:, 1:]\n            logits = model(latent_vector, labels, input_tokens)\n            logits_flat = logits.reshape(-1, model.vocab_size)\n            targets_flat = target_tokens.reshape(-1)\n            loss = criterion(logits_flat, targets_flat)\n\n            # Sentiment accuracy\n            generated_tokens = model.sample(latent_vector, labels)\n            sentiment_preds = classifier(generated_tokens)\n            pred_labels = torch.argmax(sentiment_preds, dim=1)\n            true_labels = torch.argmax(labels, dim=1)\n            correct = (pred_labels == true_labels).float().sum().item()\n\n            epoch_loss += loss.item()\n            total_correct += correct\n            total_samples += batch_size\n\n            for feat in features:\n                feat.clear()\n\n    return epoch_loss / len(iterator), total_correct / total_samples\n\n# Updated Generate Text Samples\ndef generate_text_samples(model, dataset, n_samples=10, temperature=1.0, device=None, epoch=0):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    latent_vectors = torch.randn(n_samples, model.latent_dim).to(device)\n    sentiment_labels = torch.zeros(n_samples, model.output_dim)\n    for i in range(n_samples):\n        class_idx = i % model.output_dim\n        sentiment_labels[i, class_idx] = 1\n    sentiment_labels = sentiment_labels.to(device)\n\n    token_indices = model.sample(latent_vectors, sentiment_labels, temperature=temperature)\n    generated_texts = []\n    for sample_idx in range(n_samples):\n        tokens = [dataset.idx_to_word.get(token_idx.item(), \"<UNK>\")\n                  for token_idx in token_indices[sample_idx]\n                  if dataset.idx_to_word.get(token_idx.item()) not in ['<PAD>', '<START>', '<END>']]\n        unique_ratio = len(set(tokens)) / len(tokens) if tokens else 0\n        generated_texts.append(\" \".join(tokens))\n\n    sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive', 3: 'Irrelevant'}\n    print(\"\\nGenerated Text Samples:\")\n    for i, text in enumerate(generated_texts):\n        sentiment_idx = i % model.output_dim\n        sentiment = sentiment_names[sentiment_idx]\n        print(f\"\\nSample {i+1} ({sentiment}):\")\n        print(text)\n        print(f\"Unique token ratio: {unique_ratio:.3f}\")\n\n    return generated_texts\n\n# Load Dataset\ndef load_dataset(file_path):\n    \"\"\"Load dataset from CSV file like in sentiment classification\"\"\"\n    try:\n        df = pd.read_csv(file_path, header=None)\n\n        # Assign column names based on position\n        if len(df.columns) >= 4:\n            df.columns = ['ID', 'Platform', 'Sentiment', 'Text'] + [f'Extra_{i}' for i in range(len(df.columns) - 4)]\n        else:\n            print(f\"Warning: File {file_path} has only {len(df.columns)} columns. Expected at least 4.\")\n            column_names = ['ID', 'Platform', 'Sentiment', 'Text']\n            df.columns = column_names[:len(df.columns)]\n            if len(df.columns) < 4:\n                print(\"Error: Text column is missing from the dataset.\")\n                return None\n\n        # Map sentiment labels to numerical values\n        sentiment_map = {'Positive': 2, 'Neutral': 1, 'Negative': 0, 'Irrelevant': 3}\n        df['target'] = df['Sentiment'].map(sentiment_map)\n\n        # Check if any NaN values in target column\n        if df['target'].isna().any():\n            missing_labels = df[df['target'].isna()]['Sentiment'].unique()\n            print(f\"Warning: Found unmapped sentiment labels: {missing_labels}\")\n            df.loc[df['target'].isna(), 'target'] = 1  # Default to 'Neutral'\n\n        return df\n    except Exception as e:\n        print(f\"Error loading dataset from {file_path}: {e}\")\n        return None\n\n# Main Function\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    train_file = '/kaggle/input/twitter-dataset/twitter_training.csv'\n    val_file = '/kaggle/input/twitter-dataset/twitter_validation.csv'\n    classifier_file = '/kaggle/input/social_media_sentiment_classifier/pytorch/default/1/social_media_sentiment_model.pt'\n    vocab_file = 'vocab.json'\n\n    for file_path in [train_file, val_file, classifier_file]:\n        if not os.path.exists(file_path):\n            print(f\"File {file_path} not found!\")\n            return\n\n    train_df = load_dataset(train_file)\n    val_df = load_dataset(val_file)\n\n    if train_df is None or val_df is None:\n        print(\"Error loading datasets. Exiting.\")\n        return\n\n    print(f\"Training dataset: {len(train_df)} rows\")\n    print(f\"Validation dataset: {len(val_df)} rows\")\n\n    # Check class distribution\n    sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive', 3: 'Irrelevant'}\n    print(\"\\nClass distribution in training set:\")\n    class_counts = train_df['target'].value_counts()\n    for idx, count in class_counts.items():\n        print(f\"{sentiment_names[idx]}: {count} ({count/len(train_df)*100:.2f}%)\")\n\n    X_train, y_train = train_df['Text'].values, train_df['target'].values.astype(int)\n    X_val, y_val = val_df['Text'].values, val_df['target'].values.astype(int)\n\n    max_length = 50\n    train_dataset = TextGenerationDataset(X_train, y_train, max_len=max_length)\n    vocab = train_dataset.vocab\n    vocab_size = train_dataset.vocab_size\n    print(f\"Vocabulary size: {vocab_size}\")\n    val_dataset = TextGenerationDataset(X_val, y_val, vocab=vocab, max_len=max_length)\n\n    import json\n    with open(vocab_file, 'w') as f:\n        json.dump(vocab, f)\n    print(f\"Saved vocabulary to {vocab_file}\")\n\n    batch_size = 128\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    embedding_dim = 100\n    hidden_dim = 256\n    latent_dim = 512\n    output_dim = 4\n    n_layers = 2\n    dropout = 0.5\n\n    pretrained_vocab_size = 40338\n    classifier = SentimentClassifier(pretrained_vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout).to(device)\n    classifier.load_state_dict(torch.load(classifier_file, map_location=device), strict=False)\n# After loading the classifier\n    classifier.eval()\n    val_texts, val_labels = [], []\n    for texts, labels in val_loader:\n        val_texts.append(texts)\n        val_labels.append(torch.argmax(labels, dim=1))\n    val_texts = torch.cat(val_texts).to(device)\n    val_labels = torch.cat(val_labels).to(device)\n\n    with torch.no_grad():\n        preds = classifier(val_texts)\n        pred_labels = torch.argmax(preds, dim=1)\n        classifier_acc = (pred_labels == val_labels).float().mean().item()\n    print(f\"Classifier accuracy on real validation data: {classifier_acc:.3f}\")\n\n    hooks, features = classifier.register_hooks([0, 1])\n\n    generator = TextGenerator(\n        vocab_size,\n        embedding_dim=100,\n        hidden_dim=256,\n        latent_dim=512,\n        output_dim=4,\n        max_length=50,\n        num_layers=2,\n        num_heads=4,\n        dropout=0.1\n    ).to(device)\n    optimizer = optim.Adam(generator.parameters(), lr=0.0001)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    output_dir = \"generated_texts\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    n_epochs = 30\n    best_valid_loss = float('inf')\n    patience = 8\n    patience_counter = 0\n    train_losses, val_losses, sent_losses, cos_losses, orth_losses, sentiment_accs = [], [], [], [], [], []\n\n    print(\"\\nStarting training...\")\n    for epoch in range(n_epochs):\n        train_metrics = train_generator(generator, train_loader, optimizer, criterion, classifier, device,\n                                        lambda_sent=0.5, lambda_cos=0.5, lambda_orth=0.5, features=features)  # Updated lambdas\n        val_loss, sentiment_acc = evaluate_generator(generator, val_loader, criterion, classifier, device, features)\n\n        train_loss, train_sent_loss, train_cos_loss, train_orth_loss = train_metrics\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        sent_losses.append(train_sent_loss)\n        cos_losses.append(train_cos_loss)\n        orth_losses.append(train_orth_loss)\n        sentiment_accs.append(sentiment_acc)\n\n        print(f'Epoch: {epoch+1}/{n_epochs}')\n        print(f'Train Loss: {train_loss:.3f}, Sent Loss: {train_sent_loss:.3f}, Cos Loss: {train_cos_loss:.3f}, Orth Loss: {train_orth_loss:.3f}')\n        print(f'Val Loss: {val_loss:.3f}')\n        print(f'Sentiment Accuracy: {sentiment_acc:.3f}')\n\n        if val_loss < best_valid_loss:\n            best_valid_loss = val_loss\n            patience_counter = 0\n            torch.save(generator.state_dict(), f'{output_dir}/generator_best.pt')\n            print(\"Saved best model checkpoint.\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                generator.load_state_dict(torch.load(f'{output_dir}/generator_best.pt'))\n                break\n\n        if (epoch + 1) % 5 == 0 or epoch == n_epochs - 1:\n            generated_texts = generate_text_samples(generator, train_dataset, n_samples=8, temperature=1.0, device=device, epoch=epoch)\n            with open(f\"{output_dir}/generated_texts_epoch_{epoch+1}.txt\", \"w\", encoding=\"utf-8\") as f:\n                sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive', 3: 'Irrelevant'}\n                f.write(f\"Epoch {epoch+1} Generated Texts:\\n\\n\")\n                for i, text in enumerate(generated_texts):\n                    sentiment_idx = i % output_dim\n                    sentiment = sentiment_names[sentiment_idx]\n                    f.write(f\"Sample {i+1} ({sentiment}):\\n{text}\\n\\n\")\n            torch.save(generator.state_dict(), f'{output_dir}/generator_epoch_{epoch+1}.pt')\n\n    plt.figure(figsize=(12, 10))\n    plt.subplot(3, 1, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.title('Generator Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(3, 1, 2)\n    plt.plot(sentiment_accs, label='Sentiment Accuracy')\n    plt.title('Sentiment Classification Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(3, 1, 3)\n    plt.plot(cos_losses, label='Cosine Similarity Loss')\n    plt.plot(orth_losses, label='Orthogonality Loss')\n    plt.title('Feature Losses')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(f\"{output_dir}/training_progress.png\")\n\n    print(\"\\nTraining completed.\")\n    print(f\"Best validation loss: {best_valid_loss:.3f}\")\n    generator.load_state_dict(torch.load(f'{output_dir}/generator_best.pt'))\n    final_texts = generate_text_samples(generator, train_dataset, n_samples=16, temperature=1.0, device=device)\n    with open(f\"{output_dir}/final_generated_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n        sentiment_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive', 3: 'Irrelevant'}\n        f.write(\"Final Generated Texts:\\n\\n\")\n        for i, text in enumerate(final_texts):\n            sentiment_idx = i % output_dim\n            sentiment = sentiment_names[sentiment_idx]\n            f.write(f\"Sample {i+1} ({sentiment}):\\n{text}\\n\\n\")\n\n    for hook in hooks:\n        hook.remove()\n\n    print(f\"\\nText generation complete. Results saved to {output_dir}.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T22:19:18.788401Z","iopub.execute_input":"2025-04-26T22:19:18.788690Z","execution_failed":"2025-04-26T22:23:43.446Z"}},"outputs":[],"execution_count":null}]}